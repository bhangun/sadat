

---

## Core Components

### 1. Core Domain Model

```java
// inference-core/src/main/java/com/enterprise/inference/core/domain/




```

### 2. Core Port Definitions (SPI)

```java
// inference-core/src/main/java/com/enterprise/inference/core/ports/outbound/



```

### 3. Model Router & Selection Policy

```java
// inference-core/src/main/java/com/enterprise/inference/core/service/


```

### 4. Runner Factory with Warm Pool

```java
// inference-core/src/main/java/com/enterprise/inference/core/service/

```

---

## Implementation Guide

### 5. Adapter Implementations

#### GGUF/llama.cpp Adapter

```java
// inference-adapter-gguf/src/main/java/com/enterprise/inference/adapter/gguf/

```

#### ONNX Runtime Adapter

```java
// inference-adapter-onnx/src/main/java/com/enterprise/inference/adapter/onnx/

```

#### Triton Inference Server Adapter

```java
// inference-adapter-triton/src/main/java/com/enterprise/inference/adapter/triton/

```

---

## Configuration Management

### Application Configuration

```yaml
# inference-application/src/main/resources/application.yml

# Quarkus Configuration
quarkus:
  application:
    name: inference-platform
  
  # HTTP Configuration
  http:
    port: 8080
    cors: true
    limits:
      max-body-size: 100M
  
  # Native Image Settings
  native:
    resources:
      includes: "**/*.json,**/*.yml"
  
  # Logging
  log:
    level: INFO
    category:
      "com.enterprise.inference":
        level: DEBUG
    console:
      format: "%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n"
  
  # OpenTelemetry
  otel:
    enabled: true
    exporter:
      otlp:
        endpoint: http://otel-collector:4317
  
  # Security
  oidc:
    enabled: true
    auth-server-url: ${KEYCLOAK_URL:http://keycloak:8080}/realms/inference
    client-id: inference-platform
    credentials:
      secret: ${OIDC_CLIENT_SECRET}
  
  # Database (for metadata)
  datasource:
    db-kind: postgresql
    username: ${DB_USER:inference}
    password: ${DB_PASSWORD}
    jdbc:
      url: ${DB_URL:jdbc:postgresql://postgres:5432/inference_metadata}
      max-size: 20
      min-size: 5
  
  # Flyway for schema migration
  flyway:
    migrate-at-start: true
    baseline-on-migrate: true

# Inference Platform Configuration
inference:
  # Model Repository
  repository:
    type: s3  # or minio, local
    s3:
      endpoint: ${S3_ENDPOINT:https://s3.amazonaws.com}
      bucket: ${S3_BUCKET:inference-models}
      region: ${S3_REGION:us-east-1}
      access-key: ${S3_ACCESS_KEY}
      secret-key: ${S3_SECRET_KEY}
    cache:
      enabled: true
      max-size-gb: 50
      eviction-policy: lru
      base-path: /var/cache/models
  
  # Default Runner Selection
  default-runners: ["onnx", "gguf", "triton"]
  
  # Selection Policy
  selection-policy:
    type: latency-optimized  # or cost-optimized, balanced
    scoring:
      device-match-weight: 50
      format-match-weight: 30
      latency-weight: 25
      availability-weight: 20
      cost-weight: 10
  
  # Fallback Strategy
  fallback:
    enabled: true
    max-retries: 3
    retry-delay-ms: 1000
    exponential-backoff: true
  
  # Warm Pool
  warm-pool:
    enabled: true
    max-instances: 10
    idle-timeout-minutes: 15
    prewarm-on-startup: true
    prewarm-models:
      - "gpt-small:1.0"
      - "bert-base:2.0"
  
  # Resource Limits (per tenant)
  limits:
    max-concurrent-requests: 100
    max-queue-size: 1000
    request-timeout-seconds: 30
    rate-limit:
      requests-per-minute: 1000
      burst-size: 100
  
  # Circuit Breaker
  circuit-breaker:
    enabled: true
    failure-threshold: 5
    success-threshold: 2
    timeout-seconds: 60

# Runner-Specific Configuration
runners:
  gguf:
    enabled: ${INFERENCE_ADAPTER_GGUF:true}
    threads: ${GGUF_THREADS:8}
    use-gpu: ${GGUF_USE_GPU:false}
    gpu-layers: ${GGUF_GPU_LAYERS:32}
    warmup:
      enabled: true
  
  onnx:
    enabled: ${INFERENCE_ADAPTER_ONNX:true}
    execution-provider: ${ONNX_EP:CPUExecutionProvider}
    inter-op-threads: 1
    intra-op-threads: ${ONNX_THREADS:8}
    optimization-level: all
    memory-pattern-optimization: true
  
  triton:
    enabled: ${INFERENCE_ADAPTER_TRITON:false}
    endpoint: ${TRITON_ENDPOINT:triton.svc.cluster.local:8001}
    timeout-ms: 30000
    use-ssl: false
    max-connections: 10
    connection-timeout-ms: 5000
  
  tpu:
    enabled: ${INFERENCE_ADAPTER_TPU:false}
    project-id: ${GCP_PROJECT_ID}
    zone: ${GCP_ZONE:us-central1-a}
    tpu-name: ${TPU_NAME}

# Multi-Tenancy Configuration
tenants:
  resolver:
    type: jwt  # or header, query-param
    jwt:
      claim-name: tenant_id
  
  # Default Tenant Configuration
  default:
    max-concurrent-requests: 50
    max-models: 10
    storage-quota-gb: 100
    rate-limit:
      requests-per-minute: 500
  
  # Per-Tenant Overrides
  tenants:
    tenant-premium:
      max-concurrent-requests: 200
      max-models: 50
      storage-quota-gb: 500
      rate-limit:
        requests-per-minute: 5000
      runners:
        onnx:
          execution-provider: CUDAExecutionProvider
        triton:
          enabled: true

# Observability
observability:
  metrics:
    enabled: true
    export:
      prometheus:
        enabled: true
        path: /metrics
  
  tracing:
    enabled: true
    sampler:
      probability: 0.1  # 10% sampling
    exporter:
      jaeger:
        endpoint: ${JAEGER_ENDPOINT:http://jaeger:14250}
  
  logging:
    structured: true
    include-trace-id: true
    audit:
      enabled: true
      events:
        - model_load
        - inference_request
        - inference_failure
        - quota_exceeded
```

---

## Build Configuration

### Parent POM with Profiles

```xml
<!-- pom.xml -->
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.enterprise</groupId>
    <artifactId>inference-platform-parent</artifactId>
    <version>1.0.0-SNAPSHOT</version>
    <packaging>pom</packaging>

    <name>Enterprise Inference Platform</name>
    <description>
        Production-grade multi-format inference engine with 
        multi-tenancy support
    </description>

    <properties>
        <java.version>21</java.version>
        <maven.compiler.source>21</maven.compiler.source>
        <maven.compiler.target>21</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        
        <!-- Quarkus -->
        <quarkus.version>3.16.3</quarkus.version>
        
        <!-- Libraries -->
        <onnxruntime.version>1.19.2</onnxruntime.version>
        <grpc.version>1.62.2</grpc.version>
        <protobuf.version>3.25.3</protobuf.version>
        <caffeine.version>3.1.8</caffeine.version>
        <resilience4j.version>2.2.0</resilience4j.version>
        
        <!-- Testing -->
        <junit.version>5.10.2</junit.version>
        <testcontainers.version>1.19.7</testcontainers.version>
        <rest-assured.version>5.4.0</rest-assured.version>
    </properties>

    <modules>
        <module>inference-api</module>
        <module>inference-core</module>
        <module>inference-infrastructure</module>
        <module>inference-application</module>
        <module>inference-tests</module>
    </modules>

    <dependencyManagement>
        <dependencies>
            <!-- Quarkus BOM -->
            <dependency>
                <groupId>io.quarkus.platform</groupId>
                <artifactId>quarkus-bom</artifactId>
                <version>${quarkus.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>

            <!-- Internal Modules -->
            <dependency>
                <groupId>com.enterprise</groupId>
                <artifactId>inference-api</artifactId>
                <version>${project.version}</version>
            </dependency>
            <dependency>
                <groupId>com.enterprise</groupId>
                <artifactId>inference-core</artifactId>
                <version>${project.version}</version>
            </dependency>

            <!-- ONNX Runtime -->
            <dependency>
                <groupId>com.microsoft.onnxruntime</groupId>
                <artifactId>onnxruntime</artifactId>
                <version>${onnxruntime.version}</version>
            </dependency>

            <!-- gRPC -->
            <dependency>
                <groupId>io.grpc</groupId>
                <artifactId>grpc-bom</artifactId>
                <version>${grpc.version}</version>
                <type>pom