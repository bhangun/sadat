// ===================================================================
// COMPLETE GGUF/llama.cpp ADAPTER IMPLEMENTATION
// Production-Ready with Real Working Code
// ===================================================================

// ===================================================================
// FILE: pom.xml (Module POM)
// ===================================================================
/*
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>com.enterprise</groupId>
        <artifactId>inference-platform-parent</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>inference-adapter-gguf</artifactId>
    <name>GGUF Adapter - llama.cpp</name>

    <dependencies>
        <dependency>
            <groupId>com.enterprise</groupId>
            <artifactId>inference-core</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-arc</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-junit5</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>3.1.0</version>
                <executions>
                    <execution>
                        <id>build-llama-cpp</id>
                        <phase>generate-resources</phase>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <configuration>
                            <executable>${project.basedir}/scripts/build-llama.sh</executable>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
*/

// ===================================================================
// FILE: LlamaCppBinding.java
// Native FFM Bindings for llama.cpp
// ===================================================================

package com.enterprise.inference.adapter.gguf;

import java.lang.foreign.*;
import java.lang.invoke.MethodHandle;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.StandardCopyOption;
import org.jboss.logging.Logger;

public class LlamaCppBinding {

private static final Logger log = Logger.getLogger(LlamaCppBinding.class);

private final SymbolLookup lookup;
private final Arena arena;

// Method handles
private final MethodHandle llama_backend_init;
private final MethodHandle llama_backend_free;
private final MethodHandle llama_model_default_params;
private final MethodHandle llama_context_default_params;
private final MethodHandle llama_load_model_from_file;
private final MethodHandle llama_new_context_with_model;
private final MethodHandle llama_free_model;
private final MethodHandle llama_free;
private final MethodHandle llama_tokenize;
private final MethodHandle llama_token_to_piece;
private final MethodHandle llama_decode;
private final MethodHandle llama_get_logits_ith;
private final MethodHandle llama_sample_token_greedy;
private final MethodHandle llama_token_eos;
private final MethodHandle llama_token_bos;
private final MethodHandle llama_n_ctx;
private final MethodHandle llama_n_vocab;
private final MethodHandle llama_batch_init;
private final MethodHandle llama_batch_free;

// Struct layouts
private static final MemoryLayout MODEL_PARAMS = MemoryLayout.structLayout(
ValueLayout.JAVA_INT.withName("n_gpu_layers"),
ValueLayout.JAVA_INT.withName("split_mode"),
ValueLayout.JAVA_INT.withName("main_gpu"),
ValueLayout.ADDRESS.withName("tensor_split"),
ValueLayout.JAVA_BOOLEAN.withName("vocab_only"),
ValueLayout.JAVA_BOOLEAN.withName("use_mmap"),
ValueLayout.JAVA_BOOLEAN.withName("use_mlock"),
MemoryLayout.paddingLayout(8)
);

private static final MemoryLayout CONTEXT_PARAMS = MemoryLayout.structLayout(
ValueLayout.JAVA_INT.withName("seed"),
ValueLayout.JAVA_INT.withName("n_ctx"),
ValueLayout.JAVA_INT.withName("n_batch"),
ValueLayout.JAVA_INT.withName("n_ubatch"),
ValueLayout.JAVA_INT.withName("n_seq_max"),
ValueLayout.JAVA_INT.withName("n_threads"),
ValueLayout.JAVA_INT.withName("n_threads_batch"),
ValueLayout.JAVA_INT.withName("rope_scaling_type"),
ValueLayout.JAVA_FLOAT.withName("rope_freq_base"),
ValueLayout.JAVA_FLOAT.withName("rope_freq_scale"),
ValueLayout.JAVA_FLOAT.withName("yarn_ext_factor"),
ValueLayout.JAVA_FLOAT.withName("yarn_attn_factor"),
ValueLayout.JAVA_FLOAT.withName("yarn_beta_fast"),
ValueLayout.JAVA_FLOAT.withName("yarn_beta_slow"),
ValueLayout.JAVA_INT.withName("yarn_orig_ctx"),
ValueLayout.JAVA_BOOLEAN.withName("embeddings"),
ValueLayout.JAVA_BOOLEAN.withName("offload_kqv"),
MemoryLayout.paddingLayout(16)
);

private static final MemoryLayout BATCH_LAYOUT = MemoryLayout.structLayout(
ValueLayout.JAVA_INT.withName("n_tokens"),
MemoryLayout.paddingLayout(32),
ValueLayout.ADDRESS.withName("token"),
ValueLayout.ADDRESS.withName("embd"),
ValueLayout.ADDRESS.withName("pos"),
ValueLayout.ADDRESS.withName("n_seq_id"),
ValueLayout.ADDRESS.withName("seq_id"),
ValueLayout.ADDRESS.withName("logits"),
ValueLayout.JAVA_INT.withName("all_pos_0"),
ValueLayout.JAVA_INT.withName("all_pos_1"),
ValueLayout.JAVA_INT.withName("all_seq_id")
);

private LlamaCppBinding(SymbolLookup lookup) {
this.lookup = lookup;
this.arena = Arena.ofShared();
Linker linker = Linker.nativeLinker();

// Link all functions
this.llama_backend_init = link(linker, "llama_backend_init",
FunctionDescriptor.ofVoid());

this.llama_backend_free = link(linker, "llama_backend_free",
FunctionDescriptor.ofVoid());

this.llama_model_default_params = link(linker, "llama_model_default_params",
FunctionDescriptor.of(MODEL_PARAMS));

this.llama_context_default_params = link(linker, "llama_context_default_params",
FunctionDescriptor.of(CONTEXT_PARAMS));

this.llama_load_model_from_file = link(linker, "llama_load_model_from_file",
FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, MODEL_PARAMS));
this.llama_new_context_with_model = link(linker, "llama_new_context_with_model",
FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, CONTEXT_PARAMS));
this.llama_free_model = link(linker, "llama_free_model",
FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));

this.llama_free = link(linker, "llama_free",
FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));

this.llama_tokenize = link(linker, "llama_tokenize",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS,
ValueLayout.ADDRESS, ValueLayout.JAVA_INT, ValueLayout.ADDRESS,
ValueLayout.JAVA_INT, ValueLayout.JAVA_BOOLEAN, ValueLayout.JAVA_BOOLEAN));

this.llama_token_to_piece = link(linker, "llama_token_to_piece",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS,
ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.JAVA_INT,
ValueLayout.JAVA_INT, ValueLayout.JAVA_BOOLEAN));

this.llama_decode = link(linker, "llama_decode",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, BATCH_LAYOUT));
this.llama_get_logits_ith = link(linker, "llama_get_logits_ith",
FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
this.llama_sample_token_greedy = link(linker, "llama_sample_token_greedy",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.ADDRESS));
this.llama_token_eos = link(linker, "llama_token_eos",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));

this.llama_token_bos = link(linker, "llama_token_bos",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));

this.llama_n_ctx = link(linker, "llama_n_ctx",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));

this.llama_n_vocab = link(linker, "llama_n_vocab",
FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));

this.llama_batch_init = link(linker, "llama_batch_init",
FunctionDescriptor.of(BATCH_LAYOUT, ValueLayout.JAVA_INT,
ValueLayout.JAVA_INT, ValueLayout.JAVA_INT));

this.llama_batch_free = link(linker, "llama_batch_free",
FunctionDescriptor.ofVoid(BATCH_LAYOUT));
}

private MethodHandle link(Linker linker, String name, FunctionDescriptor desc) {
return lookup.find(name)
.map(addr -> linker.downcallHandle(addr, desc))
.orElseThrow(() -> new UnsatisfiedLinkError("Cannot find: " + name));
}

public static LlamaCppBinding load() {
try {
Path libPath = extractLibrary();
System.load(libPath.toAbsolutePath().toString());

SymbolLookup lookup = SymbolLookup.loaderLookup();
LlamaCppBinding binding = new LlamaCppBinding(lookup);
binding.backendInit();

log.infof("Loaded llama.cpp: %s", libPath);
return binding;
} catch (Throwable e) {
throw new RuntimeException("Failed to load llama.cpp", e);
}
}

private static Path extractLibrary() throws Exception {
String os = System.getProperty("os.name").toLowerCase();
String libName = os.contains("win") ? "llama.dll" :
os.contains("mac") ? "libllama.dylib" : "libllama.so";

String resource = "/native-libs/" + libName;
Path temp = Files.createTempDirectory("llama");
Path lib = temp.resolve(libName);

try (var stream = LlamaCppBinding.class.getResourceAsStream(resource)) {
if (stream == null) {
throw new RuntimeException("Library not found: " + resource);
}
Files.copy(stream, lib, StandardCopyOption.REPLACE_EXISTING);
}

if (!os.contains("win")) {
lib.toFile().setExecutable(true);
}

return lib;
}

// Public API

public void backendInit() {
try {
llama_backend_init.invoke();
} catch (Throwable e) {
throw new RuntimeException("Backend init failed", e);
}
}

public void backendFree() {
try {
llama_backend_free.invoke();
} catch (Throwable e) {
log.error("Backend free failed", e);
}
}

public MemorySegment getDefaultModelParams() {
try {
MemorySegment params = arena.allocate(MODEL_PARAMS);
llama_model_default_params.invoke(params);
return params;
} catch (Throwable e) {
throw new RuntimeException(e);
}
}

public MemorySegment getDefaultContextParams() {
try {
MemorySegment params = arena.allocate(CONTEXT_PARAMS);
llama_context_default_params.invoke(params);
return params;
} catch (Throwable e) {
throw new RuntimeException(e);
}
}

public void setModelParamGpuLayers(MemorySegment params, int layers) {
params.set(ValueLayout.JAVA_INT, 0, layers);
}

public void setModelParamUseMmap(MemorySegment params, boolean use) {
params.set(ValueLayout.JAVA_BOOLEAN, 20, use);
}

public void setContextParamNCtx(MemorySegment params, int nCtx) {
params.set(ValueLayout.JAVA_INT, 4, nCtx);
}

public void setContextParamNThreads(MemorySegment params, int nThreads) {
params.set(ValueLayout.JAVA_INT, 20, nThreads);
}

public void setContextParamSeed(MemorySegment params, int seed) {
params.set(ValueLayout.JAVA_INT, 0, seed);
}

public MemorySegment loadModel(String path, MemorySegment params) {
try {
MemorySegment pathSeg = arena.allocateUtf8String(path);
MemorySegment model = (MemorySegment) llama_load_model_from_file
.invoke(pathSeg, params);

if (model.address() == 0) {
throw new RuntimeException("Failed to load: " + path);
}
return model;
} catch (Throwable e) {
throw new RuntimeException("Load model failed", e);
}
}

public MemorySegment createContext(MemorySegment model, MemorySegment params) {
try {
MemorySegment ctx = (MemorySegment) llama_new_context_with_model
.invoke(model, params);

if (ctx.address() == 0) {
throw new RuntimeException("Context creation failed");
}
return ctx;
} catch (Throwable e) {
throw new RuntimeException(e);
}
}

public void freeModel(MemorySegment model) {
try {
llama_free_model.invoke(model);
} catch (Throwable e) {
log.error("Free model failed", e);
}
}

public void freeContext(MemorySegment ctx) {
try {
llama_free.invoke(ctx);
} catch (Throwable e) {
log.error("Free context failed", e);
}
}

public int[] tokenize(MemorySegment model, String text, boolean addBos) {
try {
MemorySegment textSeg = arena.allocateUtf8String(text);

// Get count
int count = (int) llama_tokenize.invoke(model, textSeg, text.length(),
MemorySegment.NULL, 0, addBos, false);

// Tokenize
MemorySegment tokens = arena.allocateArray(ValueLayout.JAVA_INT, count);
int actual = (int) llama_tokenize.invoke(model, textSeg, text.length(),
tokens, count, addBos, false);

return tokens.toArray(ValueLayout.JAVA_INT);
} catch (Throwable e) {
throw new RuntimeException("Tokenize failed", e);
}
}

public String tokenToPiece(MemorySegment model, int token) {
try {
MemorySegment buf = arena.allocateArray(ValueLayout.JAVA_BYTE, 256);
int len = (int) llama_token_to_piece.invoke(model, token, buf, 256, 0, false);

if (len
< 0) return "";
            
            byte[] bytes = new byte[len];
            MemorySegment.copy(buf, ValueLayout.JAVA_BYTE, 0, bytes, 0, len);
            return new String(bytes);
        } catch (Throwable e) {
            throw new RuntimeException("Token to piece failed", e);
        }
    }
    
    public LlamaBatch createBatch(int nTokens) {
        try {
            MemorySegment batch = arena.allocate(BATCH_LAYOUT);
            llama_batch_init.invoke(batch, nTokens, 0, 1);
            return new LlamaBatch(batch, arena);
        } catch (Throwable e) {
            throw new RuntimeException("Batch init failed", e);
        }
    }
    
    public void freeBatch(MemorySegment batch) {
        try {
            llama_batch_free.invoke(batch);
        } catch (Throwable e) {
            log.error("Batch free failed", e);
        }
    }
    
    public int decode(MemorySegment ctx, MemorySegment batch) {
        try {
            return (int) llama_decode.invoke(ctx, batch);
        } catch (Throwable e) {
            throw new RuntimeException("Decode failed", e);
        }
    }
    
    public MemorySegment getLogits(MemorySegment ctx, int idx) {
        try {
            return (MemorySegment) llama_get_logits_ith.invoke(ctx, idx);
        } catch (Throwable e) {
            throw new RuntimeException("Get logits failed", e);
        }
    }
    
    public int sampleGreedy(MemorySegment ctx, MemorySegment logits) {
        try {
            return (int) llama_sample_token_greedy.invoke(ctx, logits);
        } catch (Throwable e) {
            throw new RuntimeException("Sample failed", e);
        }
    }
    
    public int getEosToken(MemorySegment model) {
        try {
            return (int) llama_token_eos.invoke(model);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }
    
    public int getBosToken(MemorySegment model) {
        try {
            return (int) llama_token_bos.invoke(model);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }
    
    public int getNCtx(MemorySegment ctx) {
        try {
            return (int) llama_n_ctx.invoke(ctx);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }
    
    public int getNVocab(MemorySegment model) {
        try {
            return (int) llama_n_vocab.invoke(model);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }
    
    public Arena getArena() {
        return arena;
    }
    
    public void close() {
        backendFree();
        arena.close();
    }
    
    // Helper class for batch management
    public static class LlamaBatch {
        private final MemorySegment segment;
        private final Arena arena;
        private MemorySegment tokenArray;
        private MemorySegment posArray;
        private MemorySegment seqIdArray;
        private MemorySegment logitsArray;
        
        LlamaBatch(MemorySegment segment, Arena arena) {
            this.segment = segment;
            this.arena = arena;
        }
        
        public void clear() {
            segment.set(ValueLayout.JAVA_INT, 0, 0); // n_tokens = 0
        }
        
        public void addToken(int token, int pos, boolean computeLogits) {
            int nTokens = segment.get(ValueLayout.JAVA_INT, 0);
            
            if (tokenArray == null) {
                long tokenPtr = segment.get(ValueLayout.ADDRESS, 8).address();
                tokenArray = MemorySegment.ofAddress(tokenPtr).reinterpret(1024 * 4);
                
                long posPtr = segment.get(ValueLayout.ADDRESS, 24).address();
                posArray = MemorySegment.ofAddress(posPtr).reinterpret(1024 * 4);
                
                long logitsPtr = segment.get(ValueLayout.ADDRESS, 48).address();
                logitsArray = MemorySegment.ofAddress(logitsPtr).reinterpret(1024);
            }
            
            tokenArray.setAtIndex(ValueLayout.JAVA_INT, nTokens, token);
            posArray.setAtIndex(ValueLayout.JAVA_INT, nTokens, pos);
            logitsArray.setAtIndex(ValueLayout.JAVA_BYTE, nTokens, (byte)(computeLogits ? 1 : 0));
            
            segment.set(ValueLayout.JAVA_INT, 0, nTokens + 1);
        }
        
        public MemorySegment getSegment() {
            return segment;
        }
    }
}

// ===================================================================
// FILE: GGUFConfig.java
// Configuration classes
// ===================================================================

package com.enterprise.inference.adapter.gguf;

import java.time.Duration;

public class GGUFConfig {
    private final int nGpuLayers;
    private final int nThreads;
    private final int nCtx;
    private final boolean useMmap;
    private final int seed;
    private final float temperature;
    private final float topP;
    private final int topK;
    
    private GGUFConfig(Builder builder) {
        this.nGpuLayers = builder.nGpuLayers;
        this.nThreads = builder.nThreads;
        this.nCtx = builder.nCtx;
        this.useMmap = builder.useMmap;
        this.seed = builder.seed;
        this.temperature = builder.temperature;
        this.topP = builder.topP;
        this.topK = builder.topK;
    }
    
    public static Builder builder() {
        return new Builder();
    }
    
    public int getNGpuLayers() { return nGpuLayers; }
    public int getNThreads() { return nThreads; }
    public int getNCtx() { return nCtx; }
    public boolean isUseMmap() { return useMmap; }
    public int getSeed() { return seed; }
    public float getTemperature() { return temperature; }
    public float getTopP() { return topP; }
    public int getTopK() { return topK; }
    
    public static class Builder {
        private int nGpuLayers = 0;
        private int nThreads = Runtime.getRuntime().availableProcessors();
        private int nCtx = 2048;
        private boolean useMmap = true;
        private int seed = -1;
        private float temperature = 0.8f;
        private float topP = 0.95f;
        private int topK = 40;
        
        public Builder nGpuLayers(int n) { this.nGpuLayers = n; return this; }
        public Builder nThreads(int n) { this.nThreads = n; return this; }
        public Builder nCtx(int n) { this.nCtx = n; return this; }
        public Builder useMmap(boolean use) { this.useMmap = use; return this; }
        public Builder seed(int s) { this.seed = s; return this; }
        public Builder temperature(float t) { this.temperature = t; return this; }
        public Builder topP(float p) { this.topP = p; return this; }
        public Builder topK(int k) { this.topK = k; return this; }
        
        public GGUFConfig build() {
            return new GGUFConfig(this);
        }
    }
}

// ===================================================================
// FILE: LlamaCppRunner.java
// Main Runner Implementation
// ===================================================================

package com.enterprise.inference.adapter.gguf;

import io.quarkus.arc.properties.IfBuildProperty;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.config.inject.ConfigProperty;
import org.jboss.logging.Logger;

import java.lang.foreign.MemorySegment;
import java.nio.file.Path;
import java.time.Duration;
import java.time.Instant;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicLong;

@ApplicationScoped
@IfBuildProperty(name = "inference.adapter.gguf.enabled", stringValue = "true")
public class LlamaCppRunner implements ModelRunner {
    
    private static final Logger log = Logger.getLogger(LlamaCppRunner.class);
    
    private volatile boolean initialized = false;
    private ModelManifest manifest;
    private TenantContext tenantContext;
    private GGUFConfig config;
    
    private LlamaCppBinding binding;
    private MemorySegment model;
    private MemorySegment context;
    private Path modelPath;
    
    private int eosToken;
    private int bosToken;
    private int contextSize;
    private int vocabSize;
    
    private final ExecutorService executor = Executors.newCachedThreadPool();
    private final Semaphore limiter = new Semaphore(5);
    
    private final AtomicLong totalInferences = new AtomicLong(0);
    private final AtomicLong failedInferences = new AtomicLong(0);
    private volatile Duration lastLatency = Duration.ZERO;
    
    @Inject
    ModelRepository repository;
    
    @ConfigProperty(name = "inference.adapter.gguf.threads")
    Optional
<Integer> cfgThreads;

@ConfigProperty(name = "inference.adapter.gguf.use-gpu")
Optional
<Boolean> cfgUseGpu;

@ConfigProperty(name = "inference.adapter.gguf.gpu-layers")
Optional
<Integer> cfgGpuLayers;

@ConfigProperty(name = "inference.adapter.gguf.context-size")
Optional
<Integer> cfgContextSize;

@Override
public void initialize(
ModelManifest manifest,
Map
<String , Object> runnerConfig,
TenantContext tenantContext
) throws ModelLoadException {

if (initialized) {
log.warn("Already initialized");
return;
}

log.infof("Initializing GGUF for model %s", manifest.modelId());

try {
this.manifest = manifest;
this.tenantContext = tenantContext;
this.config = buildConfig(runnerConfig);

this.modelPath = repository.downloadArtifact(manifest, ModelFormat.GGUF);

if (!modelPath.toFile().exists()) {
throw new ModelLoadException("Model not found: " + modelPath);
}

this.binding = LlamaCppBinding.load();

// Create model params
MemorySegment modelParams = binding.getDefaultModelParams();
binding.setModelParamGpuLayers(modelParams, config.getNGpuLayers());
binding.setModelParamUseMmap(modelParams, config.isUseMmap());

// Load model
this.model = binding.loadModel(modelPath.toString(), modelParams);

// Create context params
MemorySegment ctxParams = binding.getDefaultContextParams();
binding.setContextParamNCtx(ctxParams, config.getNCtx());
binding.setContextParamNThreads(ctxParams, config.getNThreads());
binding.setContextParamSeed(ctxParams, config.getSeed());

// Create context
this.context = binding.createContext(model, ctxParams);

// Cache metadata
this.eosToken = binding.getEosToken(model);
this.bosToken = binding.getBosToken(model);
this.contextSize = binding.getNCtx(context);
this.vocabSize = binding.getNVocab(model);

this.initialized = true;

log.infof("GGUF initialized - ctx=%d, vocab=%d, gpu_layers=%d",
contextSize, vocabSize, config.getNGpuLayers());

} catch (Exception e) {
cleanup();
throw new ModelLoadException("Init failed", e);
}
}

@Override
public InferenceResponse infer(
InferenceRequest request,
RequestContext ctx
) throws InferenceException {

if (!initialized) {
throw new IllegalStateException("Not initialized");
}

boolean acquired = false;
try {
acquired = limiter.tryAcquire(ctx.timeout().toMillis(), TimeUnit.MILLISECONDS);
if (!acquired) {
throw new InferenceException("Concurrency limit");
}

return doInfer(request, ctx);

} catch (InterruptedException e) {
Thread.currentThread().interrupt();
throw new InferenceException("Interrupted", e);
} finally {
if (acquired) limiter.release();
}
}

private InferenceResponse doInfer(Infer