// ===================================================================
// Complete GGUF/llama.cpp Adapter Implementation
// Module: inference-adapter-gguf
// ===================================================================

// pom.xml for inference-adapter-gguf module
/*

*/

// ===================================================================
// 1. Native Library Binding using FFM API
// ===================================================================

package com.enterprise.inference.adapter.gguf;

// ===================================================================
// 2. Model Parameters and Configuration
// ===================================================================

package com.enterprise.inference.adapter.gguf;


// ===================================================================
// 3. Complete GGUF Runner Implementation
// ===================================================================

package com.enterprise.inference.adapter.gguf;

import com.enterprise.inference.api.InferenceRequest;
import com.enterprise.inference.api.InferenceResponse;
import com.enterprise.inference.core.domain.ModelManifest;
import com.enterprise.inference.core.domain.TenantContext;
import com.enterprise.inference.core.domain.ModelFormat;
import com.enterprise.inference.core.domain.DeviceType;
import com.enterprise.inference.core.domain.ExecutionMode;
import com.enterprise.inference.core.ports.outbound.ModelRunner;
import com.enterprise.inference.core.ports.outbound.ModelRepository;
import com.enterprise.inference.core.exceptions.InferenceException;
import com.enterprise.inference.core.exceptions.InferenceTimeoutException;
import com.enterprise.inference.core.exceptions.ModelLoadException;


// ===================================================================
// 4. Integration Tests
// ===================================================================

package com.enterprise.inference.adapter.gguf;

import com.enterprise.inference.api.InferenceRequest;
import com.enterprise.inference.api.InferenceResponse;
import com.enterprise.inference.core.domain.*;

// ===================================================================
// 5. Build Script for llama.cpp
// ===================================================================

/*

