# application.yml - Complete Configuration
quarkus:
  application:
    name: ai-agent-runtime
    
  # HTTP Configuration
  http:
    port: 8080
    cors:
      ~: true
      origins: "*"
      methods: "GET,POST,PUT,DELETE,OPTIONS"
      headers: "*"
    limits:
      max-body-size: 50M
      
  # Logging
  log:
    level: INFO
    category:
      "io.quarkus.ai.agent": DEBUG
      "io.vertx": WARN
    console:
      format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
      
  # Database (for persistent storage)
  datasource:
    db-kind: postgresql
    username: ${DB_USERNAME:postgres}
    password: ${DB_PASSWORD:postgres}
    jdbc:
      url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:agentdb}
      max-size: 20
      min-size: 5
      
  hibernate-orm:
    database:
      generation: update
    log:
      sql: false
      
  # Redis Configuration
  redis:
    hosts: redis://${REDIS_HOST:localhost}:${REDIS_PORT:6379}
    password: ${REDIS_PASSWORD:}
    timeout: 10s
    max-pool-size: 20
    
  # Reactive Messaging (for async workflows)
  reactive-messaging:
    enabled: true
    
  # Scheduler
  scheduler:
    enabled: true
    
  # Health Checks
  health:
    enabled: true
    
  # Metrics
  micrometer:
    enabled: true
    export:
      prometheus:
        enabled: true
        path: /metrics
        
  # OpenTelemetry Tracing
  otel:
    enabled: ${TRACING_ENABLED:false}
    exporter:
      otlp:
        endpoint: ${OTEL_ENDPOINT:http://localhost:4317}

# AI Agent Runtime Configuration
ai-agent:
  # Workflow Execution
  execution:
    max-concurrent-workflows: ${MAX_CONCURRENT_WORKFLOWS:100}
    workflow-timeout-seconds: ${WORKFLOW_TIMEOUT:300}
    max-retry-attempts: 3
    retry-backoff-ms: 1000
    
  # Node Execution
  node:
    default-timeout-seconds: 60
    parallel-max-concurrency: 10
    
  # LLM Configuration
  llm:
    default-provider: openai
    timeout-seconds: 60
    max-retries: 3
    rate-limit-per-minute: 60
    
    # Provider API Keys
    providers:
      openai:
        api-key: ${OPENAI_API_KEY:}
        base-url: https://api.openai.com/v1
      anthropic:
        api-key: ${ANTHROPIC_API_KEY:}
        base-url: https://api.anthropic.com/v1
      google:
        api-key: ${GOOGLE_API_KEY:}
        base-url: https://generativelanguage.googleapis.com/v1beta
      cohere:
        api-key: ${COHERE_API_KEY:}
        base-url: https://api.cohere.ai/v1
      azure:
        api-key: ${AZURE_OPENAI_KEY:}
        endpoint: ${AZURE_OPENAI_ENDPOINT:}
      aws:
        region: ${AWS_REGION:us-east-1}
        access-key: ${AWS_ACCESS_KEY:}
        secret-key: ${AWS_SECRET_KEY:}
      ollama:
        base-url: ${OLLAMA_URL:http://localhost:11434}
        
  # Memory Configuration
  memory:
    default-backend: redis
    max-history-size: 100
    cleanup-interval-minutes: 60
    
  # Tool Configuration
  tool:
    default-timeout-seconds: 30
    max-retries: 3
    rate-limit-enabled: true
    
  # Security
  security:
    api-key-header: X-API-Key
    api-keys: ${API_KEYS:}
    encryption-key: ${ENCRYPTION_KEY:}
    
  # Storage
  storage:
    type: ${STORAGE_TYPE:database} # database, file, s3
    file-path: ${STORAGE_PATH:./data}
    s3-bucket: ${S3_BUCKET:}
    
  # Orchestration
  orchestration:
    max-iterations: 10
    reflection-max: 3
    hierarchical-max-workers: 5
    
  # Monitoring
  monitoring:
    enabled: true
    log-executions: true
    track-metrics: true
    alert-on-failures: true
    
  # Dev UI
  dev-ui:
    enabled: ${DEV_UI_ENABLED:true}