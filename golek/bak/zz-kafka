// Package kafka provides Kafka-based messaging and task scheduling
package kafka

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"time"

	"github.com/segmentio/kafka-go"
	"go.uber.org/zap"

	"tech.kayys.golek/pkg/core"
	"tech.kayys.golek/pkg/engine"
)

// ============================================================================
// KAFKA MESSAGING
// ============================================================================

// KafkaMessaging provides Kafka-based messaging implementation
type KafkaMessaging struct {
	config    MessagingConfig
	logger    *zap.Logger
	writer    *kafka.Writer
	scheduler *KafkaScheduler
}

// NewKafkaMessaging creates a new Kafka messaging instance
func NewKafkaMessaging(config MessagingConfig, logger *zap.Logger) (*KafkaMessaging, error) {
	writer := &kafka.Writer{
		Addr:         kafka.TCP(config.Brokers...),
		Topic:        config.Topic,
		Balancer:     &kafka.LeastBytes{},
		BatchSize:    100,
		BatchTimeout: 10 * time.Millisecond,
		Compression:  kafka.Snappy,
	}
	
	scheduler := NewKafkaScheduler(config, logger)
	
	messaging := &KafkaMessaging{
		config:    config,
		logger:    logger,
		writer:    writer,
		scheduler: scheduler,
	}
	
	logger.Info("Kafka messaging initialized",
		zap.Strings("brokers", config.Brokers),
		zap.String("topic", config.Topic),
	)
	
	return messaging, nil
}

// GetScheduler returns the task scheduler
func (m *KafkaMessaging) GetScheduler() engine.Scheduler {
	return m.scheduler
}

// PublishEvent publishes an event to Kafka
func (m *KafkaMessaging) PublishEvent(ctx context.Context, event core.Event) error {
	payload, err := json.Marshal(event)
	if err != nil {
		return fmt.Errorf("failed to marshal event: %w", err)
	}
	
	message := kafka.Message{
		Key:   []byte(event.AggregateID()),
		Value: payload,
		Headers: []kafka.Header{
			{Key: "event-type", Value: []byte(event.EventType())},
			{Key: "event-id", Value: []byte(event.EventID())},
		},
		Time: event.OccurredAt(),
	}
	
	return m.writer.WriteMessages(ctx, message)
}

// PublishEvents publishes multiple events
func (m *KafkaMessaging) PublishEvents(ctx context.Context, events []core.Event) error {
	messages := make([]kafka.Message, len(events))
	
	for i, event := range events {
		payload, err := json.Marshal(event)
		if err != nil {
			return fmt.Errorf("failed to marshal event: %w", err)
		}
		
		messages[i] = kafka.Message{
			Key:   []byte(event.AggregateID()),
			Value: payload,
			Headers: []kafka.Header{
				{Key: "event-type", Value: []byte(event.EventType())},
				{Key: "event-id", Value: []byte(event.EventID())},
			},
			Time: event.OccurredAt(),
		}
	}
	
	return m.writer.WriteMessages(ctx, messages...)
}

// Close closes the messaging connection
func (m *KafkaMessaging) Close() error {
	if err := m.writer.Close(); err != nil {
		return err
	}
	return m.scheduler.Close()
}

// ============================================================================
// KAFKA SCHEDULER
// ============================================================================

// KafkaScheduler schedules tasks via Kafka topics
type KafkaScheduler struct {
	config          MessagingConfig
	logger          *zap.Logger
	writer          *kafka.Writer
	readers         map[string]*kafka.Reader
	executorRouting map[string]string // executorType -> topic
	mu              sync.RWMutex
}

// NewKafkaScheduler creates a new Kafka-based scheduler
func NewKafkaScheduler(config MessagingConfig, logger *zap.Logger) *KafkaScheduler {
	return &KafkaScheduler{
		config:          config,
		logger:          logger,
		readers:         make(map[string]*kafka.Reader),
		executorRouting: make(map[string]string),
	}
}

// ScheduleTask schedules a task for execution
func (s *KafkaScheduler) ScheduleTask(ctx context.Context, executorType string, task *core.NodeExecutionTask) error {
	s.logger.Debug("Scheduling task",
		zap.String("executor_type", executorType),
		zap.String("run_id", task.RunID.String()),
		zap.String("node_id", task.NodeID.String()),
	)
	
	// Get or create topic for executor type
	topic := s.getExecutorTopic(executorType)
	
	// Serialize task
	taskJSON, err := json.Marshal(task)
	if err != nil {
		return fmt.Errorf("failed to marshal task: %w", err)
	}
	
	// Create Kafka message
	message := kafka.Message{
		Topic: topic,
		Key:   []byte(task.RunID),
		Value: taskJSON,
		Headers: []kafka.Header{
			{Key: "executor-type", Value: []byte(executorType)},
			{Key: "run-id", Value: []byte(task.RunID)},
			{Key: "node-id", Value: []byte(task.NodeID)},
		},
	}
	
	// Get or create writer for this topic
	writer := s.getWriter(topic)
	
	// Send message
	if err := writer.WriteMessages(ctx, message); err != nil {
		return fmt.Errorf("failed to send task: %w", err)
	}
	
	s.logger.Debug("Task scheduled successfully",
		zap.String("topic", topic),
		zap.String("task_id", fmt.Sprintf("%s:%s", task.RunID, task.NodeID)),
	)
	
	return nil
}

// CancelTasksForRun cancels all scheduled tasks for a run
func (s *KafkaScheduler) CancelTasksForRun(ctx context.Context, runID core.WorkflowRunID) error {
	// In Kafka, we can't really cancel messages that are already queued
	// We would need to send a cancellation message or use task tokens to reject
	s.logger.Info("Cancelling tasks for run", zap.String("run_id", runID.String()))
	
	// Send cancellation event
	cancellation := map[string]any{
		"type":   "TaskCancellation",
		"run_id": runID,
		"reason": "Workflow cancelled",
	}
	
	cancelJSON, _ := json.Marshal(cancellation)
	
	message := kafka.Message{
		Topic: s.config.Topic + "-cancellations",
		Key:   []byte(runID),
		Value: cancelJSON,
	}
	
	writer := s.getWriter(s.config.Topic + "-cancellations")
	return writer.WriteMessages(ctx, message)
}

// PublishEvents publishes domain events
func (s *KafkaScheduler) PublishEvents(ctx context.Context, events []core.Event) error {
	if len(events) == 0 {
		return nil
	}
	
	messages := make([]kafka.Message, len(events))
	
	for i, event := range events {
		eventJSON, err := json.Marshal(event)
		if err != nil {
			return err
		}
		
		messages[i] = kafka.Message{
			Topic: s.config.Topic,
			Key:   []byte(event.AggregateID()),
			Value: eventJSON,
			Headers: []kafka.Header{
				{Key: "event-type", Value: []byte(event.EventType())},
				{Key: "event-id", Value: []byte(event.EventID())},
			},
			Time: event.OccurredAt(),
		}
	}
	
	writer := s.getWriter(s.config.Topic)
	return writer.WriteMessages(ctx, messages...)
}

// StartConsumer starts consuming task results
func (s *KafkaScheduler) StartConsumer(ctx context.Context, handler ResultHandler) error {
	s.logger.Info("Starting Kafka consumer for task results")
	
	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers:  s.config.Brokers,
		Topic:    s.config.Topic + "-results",
		GroupID:  s.config.GroupID,
		MinBytes: 10e3, // 10KB
		MaxBytes: 10e6, // 10MB
	})
	
	s.mu.Lock()
	s.readers["results"] = reader
	s.mu.Unlock()
	
	go func() {
		for {
			select {
			case <-ctx.Done():
				return
			default:
				msg, err := reader.FetchMessage(ctx)
				if err != nil {
					s.logger.Error("Failed to fetch message", zap.Error(err))
					continue
				}
				
				// Deserialize result
				var result core.NodeExecutionResult
				if err := json.Unmarshal(msg.Value, &result); err != nil {
					s.logger.Error("Failed to unmarshal result", zap.Error(err))
					reader.CommitMessages(ctx, msg)
					continue
				}
				
				// Handle result
				if err := handler(ctx, &result); err != nil {
					s.logger.Error("Failed to handle result", zap.Error(err))
					// Don't commit - will retry
					continue
				}
				
				// Commit message
				if err := reader.CommitMessages(ctx, msg); err != nil {
					s.logger.Error("Failed to commit message", zap.Error(err))
				}
			}
		}
	}()
	
	return nil
}

// Close closes the scheduler
func (s *KafkaScheduler) Close() error {
	s.mu.Lock()
	defer s.mu.Unlock()
	
	for _, reader := range s.readers {
		if err := reader.Close(); err != nil {
			s.logger.Error("Failed to close reader", zap.Error(err))
		}
	}
	
	if s.writer != nil {
		return s.writer.Close()
	}
	
	return nil
}

// getExecutorTopic returns or creates topic for executor type
func (s *KafkaScheduler) getExecutorTopic(executorType string) string {
	s.mu.RLock()
	topic, exists := s.executorRouting[executorType]
	s.mu.RUnlock()
	
	if exists {
		return topic
	}
	
	// Create new topic name
	topic = fmt.Sprintf("%s-executor-%s", s.config.Topic, executorType)
	
	s.mu.Lock()
	s.executorRouting[executorType] = topic
	s.mu.Unlock()
	
	return topic
}

// getWriter gets or creates a Kafka writer for a topic
func (s *KafkaScheduler) getWriter(topic string) *kafka.Writer {
	s.mu.Lock()
	defer s.mu.Unlock()
	
	if s.writer == nil || s.writer.Topic != topic {
		s.writer = &kafka.Writer{
			Addr:         kafka.TCP(s.config.Brokers...),
			Topic:        topic,
			Balancer:     &kafka.LeastBytes{},
			BatchSize:    100,
			BatchTimeout: 10 * time.Millisecond,
			Compression:  kafka.Snappy,
		}
	}
	
	return s.writer
}

// ResultHandler processes task results
type ResultHandler func(ctx context.Context, result *core.NodeExecutionResult) error

// MessagingConfig contains Kafka configuration
type MessagingConfig struct {
	Type    string
	Brokers []string
	Topic   string
	GroupID string
}

// ============================================================================
// EXECUTOR REGISTRY (PostgreSQL Implementation)
// ============================================================================

type PostgresExecutorRegistry struct {
	db     *sql.DB
	logger *zap.Logger
}

func NewPostgresExecutorRegistry(db *sql.DB, logger *zap.Logger) *PostgresExecutorRegistry {
	return &PostgresExecutorRegistry{db: db, logger: logger}
}

func (r *PostgresExecutorRegistry) Register(ctx context.Context, info *core.ExecutorInfo) error {
	query := `
		INSERT INTO executors (
			id, executor_type, communication_type, endpoint, metadata,
			capabilities, max_concurrent, status, last_heartbeat, registered_at
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
		ON CONFLICT (id) DO UPDATE SET
			endpoint = $4, metadata = $5, capabilities = $6,
			max_concurrent = $7, status = $8, last_heartbeat = $9
	`
	
	metadataJSON, _ := json.Marshal(info.Metadata)
	
	_, err := r.db.ExecContext(ctx, query,
		info.ID, info.ExecutorType, info.CommunicationType, info.Endpoint,
		metadataJSON, pq.Array(info.Capabilities), info.MaxConcurrent,
		info.Status, info.LastHeartbeat, info.RegisteredAt,
	)
	
	return err
}

func (r *PostgresExecutorRegistry) Unregister(ctx context.Context, id core.ExecutorID) error {
	query := `DELETE FROM executors WHERE id = $1`
	_, err := r.db.ExecContext(ctx, query, id)
	return err
}

func (r *PostgresExecutorRegistry) FindByType(ctx context.Context, executorType string) ([]*core.ExecutorInfo, error) {
	query := `
		SELECT id, executor_type, communication_type, endpoint, metadata,
			capabilities, max_concurrent, status, last_heartbeat, registered_at
		FROM executors
		WHERE executor_type = $1 AND status = 'ACTIVE'
	`
	
	rows, err := r.db.QueryContext(ctx, query, executorType)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	
	var executors []*core.ExecutorInfo
	
	for rows.Next() {
		var info core.ExecutorInfo
		var metadataJSON []byte
		var capabilities []string
		
		if err := rows.Scan(
			&info.ID, &info.ExecutorType, &info.CommunicationType,
			&info.Endpoint, &metadataJSON, pq.Array(&capabilities),
			&info.MaxConcurrent, &info.Status, &info.LastHeartbeat,
			&info.RegisteredAt,
		); err != nil {
			return nil, err
		}
		
		json.Unmarshal(metadataJSON, &info.Metadata)
		info.Capabilities = capabilities
		
		executors = append(executors, &info)
	}
	
	return executors, rows.Err()
}

func (r *PostgresExecutorRegistry) FindByID(ctx context.Context, id core.ExecutorID) (*core.ExecutorInfo, error) {
	query := `
		SELECT id, executor_type, communication_type, endpoint, metadata,
			capabilities, max_concurrent, status, last_heartbeat, registered_at
		FROM executors
		WHERE id = $1
	`
	
	var info core.ExecutorInfo
	var metadataJSON []byte
	var capabilities []string
	
	err := r.db.QueryRowContext(ctx, query, id).Scan(
		&info.ID, &info.ExecutorType, &info.CommunicationType,
		&info.Endpoint, &metadataJSON, pq.Array(&capabilities),
		&info.MaxConcurrent, &info.Status, &info.LastHeartbeat,
		&info.RegisteredAt,
	)
	
	if err == sql.ErrNoRows {
		return nil, fmt.Errorf("executor not found")
	}
	if err != nil {
		return nil, err
	}
	
	json.Unmarshal(metadataJSON, &info.Metadata)
	info.Capabilities = capabilities
	
	return &info, nil
}

func (r *PostgresExecutorRegistry) UpdateHeartbeat(ctx context.Context, id core.ExecutorID) error {
	query := `UPDATE executors SET last_heartbeat = $1 WHERE id = $2`
	_, err := r.db.ExecContext(ctx, query, time.Now(), id)
	return err
}

func (r *PostgresExecutorRegistry) ListActive(ctx context.Context) ([]*core.ExecutorInfo, error) {
	query := `
		SELECT id, executor_type, communication_type, endpoint, metadata,
			capabilities, max_concurrent, status, last_heartbeat, registered_at
		FROM executors
		WHERE status = 'ACTIVE' AND last_heartbeat > $1
	`
	
	// Consider executors stale if no heartbeat in 60 seconds
	staleThreshold := time.Now().Add(-60 * time.Second)
	
	rows, err := r.db.QueryContext(ctx, query, staleThreshold)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	
	var executors []*core.ExecutorInfo
	
	for rows.Next() {
		var info core.ExecutorInfo
		var metadataJSON []byte
		var capabilities []string
		
		if err := rows.Scan(
			&info.ID, &info.ExecutorType, &info.CommunicationType,
			&info.Endpoint, &metadataJSON, pq.Array(&capabilities),
			&info.MaxConcurrent, &info.Status, &info.LastHeartbeat,
			&info.RegisteredAt,
		); err != nil {
			return nil, err
		}
		
		json.Unmarshal(metadataJSON, &info.Metadata)
		info.Capabilities = capabilities
		
		executors = append(executors, &info)
	}
	
	return executors, rows.Err()
}

// ============================================================================
// DISTRIBUTED LOCK MANAGER (PostgreSQL Implementation)
// ============================================================================

type PostgresLockManager struct {
	db     *sql.DB
	logger *zap.Logger
}

func NewPostgresLockManager(db *sql.DB, logger *zap.Logger) *PostgresLockManager {
	return &PostgresLockManager{db: db, logger: logger}
}

func (m *PostgresLockManager) AcquireLock(ctx context.Context, key string, timeout time.Duration) (engine.Lock, error) {
	owner := fmt.Sprintf("node-%d", time.Now().UnixNano())
	expiresAt := time.Now().Add(timeout)
	
	// Try to acquire lock
	query := `
		INSERT INTO distributed_locks (lock_key, owner, acquired_at, expires_at)
		VALUES ($1, $2, $3, $4)
		ON CONFLICT (lock_key) DO NOTHING
		RETURNING owner
	`
	
	var lockOwner string
	err := m.db.QueryRowContext(ctx, query, key, owner, time.Now(), expiresAt).Scan(&lockOwner)
	
	if err == sql.ErrNoRows {
		// Lock is held by someone else
		return nil, fmt.Errorf("failed to acquire lock: already held")
	}
	if err != nil {
		return nil, err
	}
	
	return &PostgresLock{
		key:       key,
		owner:     owner,
		expiresAt: expiresAt,
	}, nil
}

func (m *PostgresLockManager) ReleaseLock(ctx context.Context, lock engine.Lock) error {
	pgLock := lock.(*PostgresLock)
	
	query := `DELETE FROM distributed_locks WHERE lock_key = $1 AND owner = $2`
	_, err := m.db.ExecContext(ctx, query, pgLock.key, pgLock.owner)
	
	return err
}

// PostgresLock implements engine.Lock
type PostgresLock struct {
	key       string
	owner     string
	expiresAt time.Time
}

func (l *PostgresLock) Key() string           { return l.key }
func (l *PostgresLock) ExpiresAt() time.Time  { return l.expiresAt }

// Import needed for pq.Array
import (
	"database/sql"
	"github.com/lib/pq"
)