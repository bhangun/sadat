package tech.kayys.silat.kafka;

import io.smallrye.common.annotation.Blocking;
import io.smallrye.mutiny.Uni;
import io.smallrye.reactive.messaging.kafka.Record;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.core.domain.*;
import tech.kayys.silat.core.engine.*;

import java.time.Instant;
import java.util.*;

/**
 * ============================================================================
 * KAFKA INTEGRATION
 * ============================================================================
 * 
 * Kafka-based event streaming for:
 * - Domain event publishing
 * - Task distribution to executors
 * - Result collection from executors
 * - Workflow status updates
 * 
 * Topics:
 * - workflow.events       - Domain events (event sourcing)
 * - workflow.tasks        - Task assignments for executors
 * - workflow.results      - Task results from executors
 * - workflow.status       - Status updates for real-time monitoring
 */

// ==================== EVENT PUBLISHER ====================

/**
 * Publishes domain events to Kafka
 */
@ApplicationScoped
public class KafkaEventPublisher {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaEventPublisher.class);
    
    @Inject
    @Channel("workflow-events")
    Emitter<WorkflowEventMessage> eventEmitter;
    
    /**
     * Publish domain events to Kafka
     */
    public Uni<Void> publishEvents(List<ExecutionEvent> events) {
        LOG.debug("Publishing {} events to Kafka", events.size());
        
        return Uni.join().all(
            events.stream()
                .map(this::publishEvent)
                .toList()
        ).andFailFast()
        .replaceWithVoid()
        .onFailure().invoke(throwable ->
            LOG.error("Failed to publish events to Kafka", throwable)
        );
    }
    
    private Uni<Void> publishEvent(ExecutionEvent event) {
        WorkflowEventMessage message = new WorkflowEventMessage(
            event.eventId(),
            event.runId().value(),
            extractTenantId(event),
            event.eventType(),
            event.occurredAt(),
            serializeEvent(event)
        );
        
        return Uni.createFrom().completionStage(
            eventEmitter.send(message)
        );
    }
    
    private String extractTenantId(ExecutionEvent event) {
        if (event instanceof WorkflowStartedEvent wse) {
            return wse.tenantId().value();
        }
        return "system";
    }
    
    private Map<String, Object> serializeEvent(ExecutionEvent event) {
        // Serialize event to map for Kafka
        Map<String, Object> data = new HashMap<>();
        data.put("eventType", event.eventType());
        data.put("eventId", event.eventId());
        data.put("runId", event.runId().value());
        data.put("occurredAt", event.occurredAt().toString());
        
        // Add event-specific data
        if (event instanceof NodeCompletedEvent nce) {
            data.put("nodeId", nce.nodeId().value());
            data.put("attempt", nce.attempt());
            data.put("output", nce.output());
        } else if (event instanceof NodeFailedEvent nfe) {
            data.put("nodeId", nfe.nodeId().value());
            data.put("attempt", nfe.attempt());
            data.put("error", Map.of(
                "code", nfe.error().code(),
                "message", nfe.error().message()
            ));
        }
        
        return data;
    }
}

/**
 * Event message for Kafka
 */
record WorkflowEventMessage(
    String eventId,
    String runId,
    String tenantId,
    String eventType,
    Instant occurredAt,
    Map<String, Object> eventData
) {}

// ==================== TASK PRODUCER ====================

/**
 * Produces task assignments to Kafka for executors
 */
@ApplicationScoped
public class KafkaTaskProducer {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaTaskProducer.class);
    
    @Inject
    @Channel("workflow-tasks")
    Emitter<TaskMessage> taskEmitter;
    
    /**
     * Send task to executor via Kafka
     */
    public Uni<Void> sendTask(NodeExecutionTask task, String targetExecutor) {
        LOG.debug("Sending task to Kafka: run={}, node={}, executor={}", 
            task.runId().value(), task.nodeId().value(), targetExecutor);
        
        TaskMessage message = new TaskMessage(
            generateTaskId(task),
            task.runId().value(),
            task.nodeId().value(),
            task.attempt(),
            task.token().value(),
            task.context(),
            targetExecutor,
            Instant.now()
        );
        
        return Uni.createFrom().completionStage(
            taskEmitter.send(Record.of(task.runId().value(), message))
        ).onFailure().invoke(throwable ->
            LOG.error("Failed to send task to Kafka", throwable)
        );
    }
    
    private String generateTaskId(NodeExecutionTask task) {
        return String.format("%s:%s:%d", 
            task.runId().value(), 
            task.nodeId().value(), 
            task.attempt());
    }
}

/**
 * Task message for Kafka
 */
record TaskMessage(
    String taskId,
    String runId,
    String nodeId,
    int attempt,
    String executionToken,
    Map<String, Object> context,
    String targetExecutor,
    Instant scheduledAt
) {}

// ==================== TASK CONSUMER (for Executors) ====================

/**
 * Consumes tasks from Kafka (executor side)
 * This would typically be in the executor SDK
 */
@ApplicationScoped
public class KafkaTaskConsumer {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaTaskConsumer.class);
    
    @Inject
    ExecutorTaskHandler taskHandler;
    
    /**
     * Consume tasks from Kafka
     */
    @Incoming("workflow-tasks")
    @Blocking
    public void consumeTask(TaskMessage task) {
        LOG.info("Received task from Kafka: {}", task.taskId());
        
        try {
            // Convert to domain object
            NodeExecutionTask executionTask = new NodeExecutionTask(
                WorkflowRunId.of(task.runId()),
                NodeId.of(task.nodeId()),
                task.attempt(),
                new ExecutionToken(
                    task.executionToken(),
                    WorkflowRunId.of(task.runId()),
                    NodeId.of(task.nodeId()),
                    task.attempt(),
                    Instant.now().plusSeconds(3600)
                ),
                task.context()
            );
            
            // Hand off to executor
            taskHandler.executeTask(executionTask)
                .subscribe().with(
                    result -> LOG.info("Task completed: {}", task.taskId()),
                    error -> LOG.error("Task failed: {}", task.taskId(), error)
                );
            
        } catch (Exception e) {
            LOG.error("Failed to process task: {}", task.taskId(), e);
        }
    }
}

// ==================== RESULT PRODUCER (for Executors) ====================

/**
 * Produces task results to Kafka (executor side)
 */
@ApplicationScoped
public class KafkaResultProducer {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaResultProducer.class);
    
    @Inject
    @Channel("workflow-results")
    Emitter<TaskResultMessage> resultEmitter;
    
    /**
     * Send task result to engine via Kafka
     */
    public Uni<Void> sendResult(NodeExecutionResult result) {
        LOG.debug("Sending result to Kafka: run={}, node={}, status={}", 
            result.runId().value(), result.nodeId().value(), result.status());
        
        TaskResultMessage message = new TaskResultMessage(
            result.runId().value(),
            result.nodeId().value(),
            result.attempt(),
            result.status().name(),
            result.output(),
            result.error() != null ? Map.of(
                "code", result.error().code(),
                "message", result.error().message()
            ) : null,
            result.executionToken().value(),
            Instant.now()
        );
        
        return Uni.createFrom().completionStage(
            resultEmitter.send(Record.of(result.runId().value(), message))
        ).onFailure().invoke(throwable ->
            LOG.error("Failed to send result to Kafka", throwable)
        );
    }
}

/**
 * Task result message for Kafka
 */
record TaskResultMessage(
    String runId,
    String nodeId,
    int attempt,
    String status,
    Map<String, Object> output,
    Map<String, String> error,
    String executionToken,
    Instant completedAt
) {}

// ==================== RESULT CONSUMER (for Engine) ====================

/**
 * Consumes task results from Kafka (engine side)
 */
@ApplicationScoped
public class KafkaResultConsumer {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaResultConsumer.class);
    
    @Inject
    WorkflowRunManager runManager;
    
    /**
     * Consume task results from Kafka
     */
    @Incoming("workflow-results")
    @Blocking
    public void consumeResult(TaskResultMessage result) {
        LOG.info("Received result from Kafka: run={}, node={}", 
            result.runId(), result.nodeId());
        
        try {
            // Convert to domain object
            NodeExecutionResult executionResult = new NodeExecutionResult(
                WorkflowRunId.of(result.runId()),
                NodeId.of(result.nodeId()),
                result.attempt(),
                NodeExecutionStatus.valueOf(result.status()),
                result.output(),
                result.error() != null ? new ErrorInfo(
                    result.error().get("code"),
                    result.error().get("message"),
                    "",
                    Map.of()
                ) : null,
                new ExecutionToken(
                    result.executionToken(),
                    WorkflowRunId.of(result.runId()),
                    NodeId.of(result.nodeId()),
                    result.attempt(),
                    Instant.now().plusSeconds(3600)
                )
            );
            
            // Submit to run manager
            runManager.handleNodeResult(
                WorkflowRunId.of(result.runId()),
                executionResult
            ).subscribe().with(
                v -> LOG.info("Result processed: run={}, node={}", 
                    result.runId(), result.nodeId()),
                error -> LOG.error("Failed to process result", error)
            );
            
        } catch (Exception e) {
            LOG.error("Failed to consume result", e);
        }
    }
}

// ==================== STATUS UPDATES ====================

/**
 * Publishes workflow status updates to Kafka
 */
@ApplicationScoped
public class KafkaStatusPublisher {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaStatusPublisher.class);
    
    @Inject
    @Channel("workflow-status")
    Emitter<StatusUpdateMessage> statusEmitter;
    
    /**
     * Publish status update
     */
    public Uni<Void> publishStatusUpdate(
            WorkflowRunId runId, 
            RunStatus status,
            String message) {
        
        StatusUpdateMessage update = new StatusUpdateMessage(
            runId.value(),
            status.name(),
            message,
            Instant.now()
        );
        
        return Uni.createFrom().completionStage(
            statusEmitter.send(Record.of(runId.value(), update))
        ).onFailure().invoke(throwable ->
            LOG.error("Failed to publish status update", throwable)
        );
    }
}

/**
 * Status update message for Kafka
 */
record StatusUpdateMessage(
    String runId,
    String status,
    String message,
    Instant timestamp
) {}

/**
 * Consumes status updates (for monitoring/dashboards)
 */
@ApplicationScoped
public class KafkaStatusConsumer {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaStatusConsumer.class);
    
    @Inject
    StatusNotificationService notificationService;
    
    /**
     * Consume status updates
     */
    @Incoming("workflow-status")
    public void consumeStatusUpdate(StatusUpdateMessage update) {
        LOG.debug("Received status update: run={}, status={}", 
            update.runId(), update.status());
        
        // Forward to notification service (WebSocket, SSE, etc.)
        notificationService.notifyStatusChange(
            update.runId(),
            update.status(),
            update.message()
        );
    }
}

// ==================== EVENT CONSUMER (for Projections) ====================

/**
 * Consumes domain events to build read models
 */
@ApplicationScoped
public class KafkaEventConsumer {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaEventConsumer.class);
    
    @Inject
    EventProjectionService projectionService;
    
    /**
     * Consume domain events for projections
     */
    @Incoming("workflow-events")
    @Blocking
    public void consumeEvent(WorkflowEventMessage event) {
        LOG.trace("Processing event: type={}, runId={}", 
            event.eventType(), event.runId());
        
        try {
            // Project event to read models
            projectionService.project(event)
                .subscribe().with(
                    v -> LOG.trace("Event projected: {}", event.eventId()),
                    error -> LOG.error("Failed to project event", error)
                );
            
        } catch (Exception e) {
            LOG.error("Failed to consume event", e);
        }
    }
}

// ==================== DEAD LETTER QUEUE ====================

/**
 * Handles failed messages
 */
@ApplicationScoped
public class KafkaDeadLetterHandler {
    
    private static final Logger LOG = LoggerFactory.getLogger(KafkaDeadLetterHandler.class);
    
    @Inject
    @Channel("workflow-dlq")
    Emitter<DeadLetterMessage> dlqEmitter;
    
    /**
     * Send failed message to DLQ
     */
    public Uni<Void> sendToDeadLetter(
            String originalTopic,
            String originalKey,
            Object originalValue,
            Throwable error) {
        
        LOG.warn("Sending message to DLQ: topic={}, error={}", 
            originalTopic, error.getMessage());
        
        DeadLetterMessage dlqMessage = new DeadLetterMessage(
            originalTopic,
            originalKey,
            originalValue.toString(),
            error.getMessage(),
            Instant.now()
        );
        
        return Uni.createFrom().completionStage(
            dlqEmitter.send(dlqMessage)
        );
    }
    
    /**
     * Process dead letter messages
     */
    @Incoming("workflow-dlq")
    @Blocking
    public void processDea dLetter(DeadLetterMessage message) {
        LOG.error("Dead letter message: topic={}, key={}, error={}", 
            message.originalTopic(), message.originalKey(), message.errorMessage());
        
        // Store in database for analysis
        // Alert operations team
        // Attempt retry with exponential backoff
    }
}

/**
 * Dead letter message
 */
record DeadLetterMessage(
    String originalTopic,
    String originalKey,
    String originalValue,
    String errorMessage,
    Instant timestamp
) {}

// ==================== SUPPORTING SERVICES ====================

/**
 * Executor task handler interface
 */
@ApplicationScoped
public class ExecutorTaskHandler {
    
    @Inject
    WorkflowExecutorRegistry executorRegistry;
    
    public Uni<NodeExecutionResult> executeTask(NodeExecutionTask task) {
        // Find appropriate executor and execute
        return executorRegistry.getExecutor(task.nodeId())
            .flatMap(executor -> executor.execute(task));
    }
}

/**
 * Status notification service
 */
@ApplicationScoped
public class StatusNotificationService {
    
    private static final Logger LOG = LoggerFactory.getLogger(StatusNotificationService.class);
    
    public void notifyStatusChange(String runId, String status, String message) {
        LOG.info("Status change notification: run={}, status={}", runId, status);
        
        // Send to WebSocket clients
        // Send to SSE streams
        // Send to webhooks
    }
}

/**
 * Event projection service
 */
@ApplicationScoped
public class EventProjectionService {
    
    @Inject
    tech.kayys.silat.core.persistence.WorkflowRunRepository repository;
    
    public Uni<Void> project(WorkflowEventMessage event) {
        // Project event to read model
        // Update materialized views
        // Update analytics tables
        return Uni.createFrom().voidItem();
    }
}

/**
 * Workflow executor registry
 */
@ApplicationScoped
public class WorkflowExecutorRegistry {
    
    private final Map<String, WorkflowExecutor> executors = new HashMap<>();
    
    public void registerExecutor(String nodeType, WorkflowExecutor executor) {
        executors.put(nodeType, executor);
    }
    
    public Uni<WorkflowExecutor> getExecutor(NodeId nodeId) {
        // Find executor for node type
        // For now, return dummy
        return Uni.createFrom().nullItem();
    }
}

/**
 * Workflow executor interface
 */
public interface WorkflowExecutor {
    Uni<NodeExecutionResult> execute(NodeExecutionTask task);
    String executorType();
}

// ==================== KAFKA CONFIGURATION ====================

/**
 * Kafka serializers and deserializers
 */
@ApplicationScoped
public class WorkflowEventSerde implements org.apache.kafka.common.serialization.Serde<WorkflowEventMessage> {
    
    @Inject
    com.fasterxml.jackson.databind.ObjectMapper objectMapper;
    
    @Override
    public org.apache.kafka.common.serialization.Serializer<WorkflowEventMessage> serializer() {
        return (topic, data) -> {
            try {
                return objectMapper.writeValueAsBytes(data);
            } catch (Exception e) {
                throw new RuntimeException("Failed to serialize", e);
            }
        };
    }
    
    @Override
    public org.apache.kafka.common.serialization.Deserializer<WorkflowEventMessage> deserializer() {
        return (topic, data) -> {
            try {
                return objectMapper.readValue(data, WorkflowEventMessage.class);
            } catch (Exception e) {
                throw new RuntimeException("Failed to deserialize", e);
            }
        };
    }
}